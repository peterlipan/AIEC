import torch
import torch.nn as nn
import torch.distributed as dist


class GatherLayer(torch.autograd.Function):
    """Gather tensors from all process, supporting backward propagation."""

    @staticmethod
    def forward(ctx, input):
        ctx.save_for_backward(input)
        output = [torch.zeros_like(input) for _ in range(dist.get_world_size())]
        dist.all_gather(output, input)
        return tuple(output)

    @staticmethod
    def backward(ctx, *grads):
        (input,) = ctx.saved_tensors
        grad_out = torch.zeros_like(input)
        grad_out[:] = grads[dist.get_rank()]
        return grad_out


class ProbabilityLoss(nn.Module):
    def __init__(self):
        super(ProbabilityLoss, self).__init__()
        self.softmax = nn.Softmax(dim=-1)
        self.criterion = nn.KLDivLoss(reduction='batchmean')

    def forward(self, logits1, logits2):
        assert logits1.size() == logits2.size()
        softmax1 = self.softmax(logits1)
        softmax2 = self.softmax(logits2)

        probability_loss = self.criterion(softmax1.log(), softmax2)
        return probability_loss


class BatchLoss(nn.Module):
    def __init__(self, batch_size, world_size):
        super(BatchLoss, self).__init__()
        self.batch_size = batch_size
        self.world_size = world_size

    def forward(self, activations, ema_activations):
        assert activations.size() == ema_activations.size()
        N = self.batch_size * self.world_size
        # gather data from all the processes
        if self.world_size > 1:
            activations = torch.cat(GatherLayer.apply(activations), dim=0)
            ema_activations = torch.cat(GatherLayer.apply(ema_activations), dim=0)
        # reshape as N*C
        activations = activations.view(N, -1)
        ema_activations = ema_activations.view(N, -1)

        # form N*N similarity matrix
        similarity = activations.mm(activations.t())
        norm = torch.norm(similarity, 2, 1).view(-1, 1)
        similarity = similarity / norm

        ema_similarity = ema_activations.mm(ema_activations.t())
        ema_norm = torch.norm(ema_similarity, 2, 1).view(-1, 1)
        ema_similarity = ema_similarity / ema_norm

        batch_loss = (similarity - ema_similarity) ** 2 / N
        return batch_loss


class ChannelLoss(nn.Module):
    def __init__(self, batch_size, world_size):
        super(ChannelLoss, self).__init__()
        self.batch_size = batch_size
        self.world_size = world_size

    def forward(self, activations, ema_activations):
        assert activations.size() == ema_activations.size()
        N = self.batch_size * self.world_size
        # gather data from all the processes
        if self.world_size > 1:
            activations = torch.cat(GatherLayer.apply(activations), dim=0)
            ema_activations = torch.cat(GatherLayer.apply(ema_activations), dim=0)
        # reshape as N*C
        activations = activations.view(N, -1)
        ema_activations = ema_activations.view(N, -1)

        # form C*C channel-wise similarity matrix
        similarity = activations.t().mm(activations)
        norm = torch.norm(similarity, 2, 1).view(-1, 1)
        similarity = similarity / norm

        ema_similarity = ema_activations.t().mm(ema_activations)
        ema_norm = torch.norm(ema_similarity, 2, 1).view(-1, 1)
        ema_similarity = ema_similarity / ema_norm

        channel_loss = (similarity - ema_similarity) ** 2 / N
        return channel_loss
